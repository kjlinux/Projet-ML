{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Water IA - Analyse et Modélisation\\n",
    "\\n",
    "Ce notebook présente l'analyse complète des données de consommation d'eau, visant à comprendre la baisse des revenus FDE et à modéliser le comportement des clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation de données\\n",
    "import pandas as pd\\n",
    "import numpy as np\\n",
    "import os\\n",
    "\\n",
    "# Visualisation\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "import plotly.express as px\\n",
    "\\n",
    "# Prétraitement\\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\\n",
    "from sklearn.impute import SimpleImputer\\n",
    "from sklearn.pipeline import Pipeline\\n",
    "\\n",
    "# Modèles de Régression\\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\\n",
    "from sklearn.decomposition import PCA\\n",
    "from sklearn.cross_decomposition import PLSRegression\\n",
    "\\n",
    "# Modèles de Classification\\n",
    "from sklearn.linear_model import LogisticRegression\\n",
    "from sklearn.tree import DecisionTreeClassifier\\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\\n",
    "import xgboost as xgb\\n",
    "from sklearn.neighbors import KNeighborsClassifier\\n",
    "\\n",
    "# Clustering\\n",
    "from sklearn.cluster import KMeans\\n",
    "\\n",
    "# Métriques d'évaluation\\n",
    "from sklearn.metrics import (\\n",
    "    mean_squared_error, r2_score, mean_absolute_error,\\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,\\n",
    "    roc_curve, roc_auc_score\\n",
    ")\\n",
    "\\n",
    "# Gestion des classes déséquilibrées\\n",
    "from imblearn.over_sampling import SMOTE\\n",
    "\\n",
    "# Ignorer les avertissements pour une meilleure lisibilité\\n",
    "import warnings\\n",
    "warnings.filterwarnings('ignore')\\n",
    "\\n",
    "# Configuration de l'affichage\\n",
    "pd.set_option('display.max_columns', None)\\n",
    "sns.set_style('whitegrid')\\n",
    "\\n",
    "print(\\"Bibliothèques importées avec succès.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Préparation des Données\\n",
    "\\n",
    "Nous commençons par charger l'ensemble des fichiers de données. Chaque fichier `.txt` correspond à une Direction Régionale (DR). Conformément au plan, nous allons :\\n",
    "1.  Identifier tous les fichiers de données dans le répertoire `datasets/`.\\n",
    "2.  Charger chaque fichier dans un DataFrame `pandas`, en supposant un format CSV.\\n",
    "3.  Concaténer tous les DataFrames en un seul pour l'analyse globale.\\n",
    "\\n",
    "Cette approche permet de travailler sur l'intégralité des 23 millions d'observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le chemin vers le dossier des données\\n",
    "data_path = 'datasets/'\\n",
    "\\n",
    "# Lister les fichiers à charger (en s'assurant que le dossier existe)\\n",
    "if not os.path.exists(data_path):\\n",
    "    print(f\\"Le dossier '{data_path}' n'existe pas. Veuillez le créer et y placer les fichiers de données.\\")\\n",
    "else:\\n",
    "    file_names = [f for f in os.listdir(data_path) if f.endswith('.txt')]\\n",
    "\\n",
    "    # Créer une liste pour stocker les DataFrames individuels\\n",
    "    list_df = []\\n",
    "\\n",
    "    # Charger chaque fichier et l'ajouter à la liste\\n",
    "    for file in file_names:\\n",
    "        try:\\n",
    "            # On suppose que le séparateur est une virgule, comme suggéré par l'en-tête fourni.\\n",
    "            # On utilise `low_memory=False` pour optimiser le chargement de gros fichiers avec des types mixtes.\\n",
    "            df_temp = pd.read_csv(\\n",
    "                os.path.join(data_path, file),\\n",
    "                sep=',',\\n",
    "                low_memory=False\\n",
    "            )\\n",
    "            list_df.append(df_temp)\\n",
    "            print(f\\"Fichier '{file}' chargé avec succès ({len(df_temp)} lignes).\\")\\n",
    "        except Exception as e:\\n",
    "            print(f\\"Erreur lors du chargement du fichier {file}: {e}\\")\\n",
    "\\n",
    "    # Concaténer tous les DataFrames s'ils ont été chargés\\n",
    "    if list_df:\\n",
    "        df = pd.concat(list_df, ignore_index=True)\\n",
    "\\n",
    "        # Afficher les informations sur le DataFrame combiné\\n",
    "        print(\\"\\\\nChargement des données terminé.\\")\\n",
    "        print(f\\"Nombre total de lignes chargées : {len(df)}\\")\\n",
    "        print(f\\"Dimensions du DataFrame : {df.shape}\\")\\n",
    "        print(\\"\\\\nAperçu des données :\\")\\n",
    "        display(df.head())\\n",
    "        print(\\"\\\\nInformations sur les colonnes et types de données :\\")\\n",
    "        df.info()\\n",
    "    else:\\n",
    "        print(\\"Aucun fichier n'a été chargé.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nettoyage et Ingénierie des Variables (Feature Engineering)\\n",
    "\\n",
    "Cette étape est cruciale. Nous allons non seulement nettoyer les données, mais aussi créer de nouvelles variables pertinentes pour améliorer la performance de nos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\\n",
    "    # 1. Correction complète des types de données\\n",
    "    print(\\"Correction des types de données...\\")\\n",
    "    date_cols = ['DATE-FACT', 'DATE-ABON', 'DATE-RESIL', 'DATE-REGLT']\\n",
    "    for col in date_cols:\\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\\n",
    "\\n",
    "    # Liste exhaustive des colonnes quantitatives\\n",
    "    quantitative_cols = [\\n",
    "        'CUBCONS', 'CUBFAC', 'FORFAIT', 'SOCIAL', 'DOMEST', 'NORMAL', 'INDUST', 'ADMINI',\\n",
    "        'MONT-SOD', 'MONT-TVA', 'MONT-FDE', 'MONT-FNE', 'MONT-ASS-TTC', 'MONT-FRAIS-CPT', 'MONT-TTC',\\n",
    "        'DELAI_REGL'\\n",
    "    ]\\n",
    "    for col in quantitative_cols:\\n",
    "        if col in df.columns:\\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\\n",
    "        else:\\n",
    "            print(f\\"Attention : La colonne quantitative '{col}' est introuvable.\\")\\n",
    "    \\n",
    "    # 2. Création de nouvelles variables (Feature Engineering)\\n",
    "    print(\\"\\\\nCréation de nouvelles variables...\\")\\n",
    "    \\n",
    "    # Ancienneté du client en jours\\n",
    "    # Nous prenons la date de facturation la plus récente comme référence\\n",
    "    latest_fact_date = df['DATE-FACT'].max()\\n",
    "    df['ANCIENNETE_JOURS'] = (latest_fact_date - df['DATE-ABON']).dt.days\\n",
    "    \\n",
    "    # Délai de paiement effectif en jours\\n",
    "    df['DELAI_PAIEMENT_JOURS'] = (df['DATE-REGLT'] - df['DATE-FACT']).dt.days\\n",
    "\\n",
    "    # Remplacer les délais négatifs (erreurs de données) par zéro\\n",
    "    df['DELAI_PAIEMENT_JOURS'] = df['DELAI_PAIEMENT_JOURS'].apply(lambda x: x if x >= 0 else 0)\\n",
    "\\n",
    "    print(\\"Nouvelles variables 'ANCIENNETE_JOURS' et 'DELAI_PAIEMENT_JOURS' créées.\\")\\n",
    "\\n",
    "    # 3. Gestion des valeurs manquantes\\n",
    "    print(\\"\\\\nAnalyse et gestion des valeurs manquantes...\\")\\n",
    "    # Imputer les valeurs numériques manquantes avec la médiane\\n",
    "    for col in df.select_dtypes(include=np.number).columns:\\n",
    "        if df[col].isnull().sum() > 0:\\n",
    "            median_val = df[col].median()\\n",
    "            df[col].fillna(median_val, inplace=True)\\n",
    "\\n",
    "    # 4. Suppression des doublons\\n",
    "    initial_rows = len(df)\\n",
    "    df.drop_duplicates(inplace=True)\\n",
    "    print(f\\"\\\\n{initial_rows - len(df)} lignes dupliquées supprimées.\\")\\n",
    "\\n",
    "    # 5. Vérification finale\\n",
    "    print(\\"\\\\nPré-traitement terminé. Informations sur le DataFrame après traitement :\\")\\n",
    "    df.info()\\n",
    "else:\\n",
    "    print(\\"Le DataFrame 'df' n'a pas été chargé.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse Exploratoire des Données (EDA)\\n",
    "\\n",
    "Maintenant que les données sont propres, nous pouvons explorer leurs caractéristiques à travers des statistiques et des visualisations pour en extraire des informations pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\\n",
    "    # 1. Statistiques descriptives\\n",
    "    print(\\"Statistiques descriptives pour les variables numériques :\\")\\n",
    "    display(df.describe().T)\\n",
    "\\n",
    "    # 2. Distribution des variables numériques clés\\n",
    "    print(\\"\\\\nDistribution de la variable cible 'MONT-FDE' et 'CUBCONS' :\\")\\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\\n",
    "    sns.histplot(df['MONT-FDE'], bins=50, kde=True, ax=axes[0])\\n",
    "    axes[0].set_title('Distribution de MONT-FDE')\\n",
    "    axes[0].set_yscale('log') # Utiliser une échelle log pour mieux voir la distribution\\n",
    "\\n",
    "    sns.histplot(df['CUBCONS'], bins=50, kde=True, ax=axes[1])\\n",
    "    axes[1].set_title('Distribution de CUBCONS')\\n",
    "    axes[1].set_yscale('log')\\n",
    "    plt.tight_layout()\\n",
    "    plt.show()\\n",
    "\\n",
    "    # 3. Analyse des variables cibles de classification\\n",
    "    print(\\"\\\\nDistribution des variables cibles 'RETARD' et 'RESILIE' :\\")\\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\\n",
    "    sns.countplot(x='RETARD', data=df, ax=axes[0])\\n",
    "    axes[0].set_title('Distribution des retards de paiement')\\n",
    "    \\n",
    "    sns.countplot(x='RESILIE', data=df, ax=axes[1])\\n",
    "    axes[1].set_title('Distribution des clients résiliés')\\n",
    "    plt.tight_layout()\\n",
    "    plt.show()\\n",
    "\\n",
    "    # 4. Matrice de corrélation\\n",
    "    print(\\"\\\\nMatrice de corrélation des variables numériques :\\")\\n",
    "    plt.figure(figsize=(18, 12))\\n",
    "    corr_matrix = df.select_dtypes(include=np.number).corr()\\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\\n",
    "    plt.title('Matrice de corrélation')\\n",
    "    plt.show()\\n",
    "\\n",
    "    # 5. Analyse temporelle de MONT-FDE\\n",
    "    print(\\"\\\\nÉvolution temporelle du MONT-FDE total par mois :\\")\\n",
    "    df['ANNEE_MOIS'] = df['DATE-FACT'].dt.to_period('M')\\n",
    "    monthly_fde = df.groupby('ANNEE_MOIS')['MONT-FDE'].sum().reset_index()\\n",
    "    monthly_fde['ANNEE_MOIS'] = monthly_fde['ANNEE_MOIS'].astype(str)\\n",
    "    \\n",
    "    plt.figure(figsize=(18, 7))\\n",
    "    sns.lineplot(x='ANNEE_MOIS', y='MONT-FDE', data=monthly_fde, marker='o')\\n",
    "    plt.title('Évolution mensuelle du MONT-FDE total (2014-2020+)')\\n",
    "    plt.xticks(rotation=45)\\n",
    "    plt.ylabel('MONT-FDE Total')\\n",
    "    plt.xlabel('Mois')\\n",
    "    plt.grid(True)\\n",
    "    plt.show()\\n",
    "\\n",
    "    # 6. Analyse segmentée par catégorie de client\\n",
    "    print(\\"\\\\nAnalyse segmentée par catégorie de client :\\")\\n",
    "    category_analysis = df.groupby('CATEGORIE')[['MONT-FDE', 'CUBCONS']].mean().reset_index()\\n",
    "    display(category_analysis)\\n",
    "\\n",
    "    fig = px.bar(category_analysis, x='CATEGORIE', y='MONT-FDE', color='CATEGORIE',\\n",
    "                 title='Montant FDE moyen par catégorie de client',\\n",
    "                 labels={'MONT-FDE': 'Montant FDE moyen', 'CATEGORIE': 'Catégorie'})\\n",
    "    fig.show()\\n",
    "else:\\n",
    "    print(\\"Le DataFrame 'df' n'a pas été chargé. Veuillez exécuter les étapes précédentes.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modélisation par Régression : Prédiction de MONT-FDE\\n",
    "\\n",
    "Nous allons maintenant construire plusieurs modèles de régression pour prédire le `MONT-FDE`. L'objectif est de comparer leurs performances et de sélectionner le meilleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\\n",
    "    # 1. Préparation des données\\n",
    "    print(\\"Préparation des données pour la modélisation de régression...\\")\\n",
    "    \\n",
    "    # Copier le DataFrame pour éviter les modifications sur l'original\\n",
    "    df_reg = df.copy()\\n",
    "    \\n",
    "    # Sélectionner les features et la cible\\n",
    "    target = 'MONT-FDE'\\n",
    "    features = df_reg.select_dtypes(include=np.number).columns.tolist()\\n",
    "    features.remove(target) # Exclure la cible des features\\n",
    "    \\n",
    "    # Gérer les colonnes non-numériques si elles sont pertinentes\\n",
    "    categorical_cols = ['CATEGORIE', 'DR'] # Exemple\\n",
    "    for col in categorical_cols:\\n",
    "        if col in df_reg.columns:\\n",
    "            le = LabelEncoder()\\n",
    "            df_reg[col] = le.fit_transform(df_reg[col].astype(str))\\n",
    "            features.append(col)\\n",
    "\\n",
    "    # Nettoyer les features non pertinentes (ex: identifiants)\\n",
    "    features = [f for f in features if 'ID' not in f.upper() and 'POLICE' not in f.upper() and 'CEN' not in f.upper()] \\n",
    "    \\n",
    "    X = df_reg[features]\\n",
    "    y = df_reg[target]\\n",
    "    \\n",
    "    # 2. Division en ensembles d'entraînement et de test\\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n",
    "    print(f\\"Données divisées : {len(X_train)} pour l'entraînement, {len(X_test)} pour le test.\\")\\n",
    "    \\n",
    "    # 3. Mise à l'échelle des features\\n",
    "    scaler = StandardScaler()\\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\\n",
    "    X_test_scaled = scaler.transform(X_test)\\n",
    "    \\n",
    "    # 4. Entraînement et évaluation des modèles\\n",
    "    # Création du pipeline pour la PCR\\n",
    "    pcr_pipeline = Pipeline([('pca', PCA()), ('linear_regression', LinearRegression())])\\n",
    "\\n",
    "    models = {\\n",
    "        'Régression Linéaire': LinearRegression(),\\n",
    "        'Ridge': Ridge(),\\n",
    "        'Lasso': Lasso(),\\n",
    "        'ElasticNet': ElasticNet(),\\n",
    "        'PCR': pcr_pipeline,\\n",
    "        'PLS': PLSRegression()\\n",
    "    }\\n",
    "    \\n",
    "    results = {}\\n",
    "    \\n",
    "    for name, model in models.items():\\n",
    "        print(f\\"\\\\nEntraînement du modèle : {name}\\")\\n",
    "        model.fit(X_train_scaled, y_train)\\n",
    "        y_pred = model.predict(X_test_scaled)\\n",
    "        \\n",
    "        # Calcul des métriques\\n",
    "        r2 = r2_score(y_test, y_pred)\\n",
    "        mse = mean_squared_error(y_test, y_pred)\\n",
    "        mae = mean_absolute_error(y_test, y_pred)\\n",
    "        results[name] = {'R²': r2, 'MSE': mse, 'MAE': mae}\\n",
    "        \\n",
    "        print(f\\"R² : {r2:.4f}\\")\\n",
    "        print(f\\"MSE : {mse:.4f}\\")\\n",
    "        print(f\\"MAE : {mae:.4f}\\")\\n",
    "        \\n",
    "    # 5. Comparaison des résultats\\n",
    "    results_df = pd.DataFrame(results).T\\n",
    "    print(\\"\\\\nTableau comparatif des performances des modèles de régression :\\")\\n",
    "    display(results_df.sort_values(by='R²', ascending=False))\\n",
    "else:\\n",
    "    print(\\"Le DataFrame 'df' n'a pas été chargé. Veuillez exécuter les étapes précédentes.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modélisation par Classification\\n",
    "\\n",
    "Nous allons maintenant aborder les tâches de classification pour prédire deux comportements clients distincts : le retard de paiement (`RETARD`) et la résiliation (`RESILIE`). Pour chaque variable cible, nous suivrons une approche structurée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Prédiction du Retard de Paiement (`RETARD`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\\n",
    "    # 1. Préparation des données pour la classification de 'RETARD'\\n",
    "    print(\\"Préparation des données pour prédire RETARD...\\")\\n",
    "    df_clf_retard = df.copy()\\n",
    "    \\n",
    "    # Définir la cible et les features\\n",
    "    target_retard = 'RETARD'\\n",
    "    # Utiliser les mêmes features que pour la régression, sauf la cible elle-même et 'RESILIE'\\n",
    "    features_clf = [f for f in features if f not in ['RETARD', 'RESILIE']]\\n",
    "    \\n",
    "    X_retard = df_clf_retard[features_clf]\\n",
    "    y_retard = df_clf_retard[target_retard]\\n",
    "\\n",
    "    # 2. Division des données\\n",
    "    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_retard, y_retard, test_size=0.3, random_state=42, stratify=y_retard)\\n",
    "\\n",
    "    # 3. Mise à l'échelle\\n",
    "    scaler_r = StandardScaler()\\n",
    "    X_train_scaled_r = scaler_r.fit_transform(X_train_r)\\n",
    "    X_test_scaled_r = scaler_r.transform(X_test_r)\\n",
    "\\n",
    "    # 4. Gestion du déséquilibre avec SMOTE\\n",
    "    print(\\"Application de SMOTE pour équilibrer les classes...\\")\\n",
    "    smote = SMOTE(random_state=42)\\n",
    "    X_train_resampled_r, y_train_resampled_r = smote.fit_resample(X_train_scaled_r, y_train_r)\\n",
    "    print(f\\"Taille des données d'entraînement avant SMOTE : {X_train_scaled_r.shape}\\")\\n",
    "    print(f\\"Taille des données d'entraînement après SMOTE : {X_train_resampled_r.shape}\\")\\n",
    "\\n",
    "    # 5. Entraînement et évaluation des modèles\\n",
    "    clf_models = {\\n",
    "        'Régression Logistique': LogisticRegression(random_state=42),\\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=10),\\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\\n",
    "        'k-NN': KNeighborsClassifier()\\n",
    "    }\\n",
    "\\n",
    "    results_clf_retard = {}\\n",
    "    for name, model in clf_models.items():\\n",
    "        print(f\\"\\\\nEntraînement du modèle : {name}\\")\\n",
    "        model.fit(X_train_resampled_r, y_train_resampled_r)\\n",
    "        y_pred_r = model.predict(X_test_scaled_r)\\n",
    "        \\n",
    "        # Métriques\\n",
    "        acc = accuracy_score(y_test_r, y_pred_r)\\n",
    "        prec = precision_score(y_test_r, y_pred_r)\\n",
    "        rec = recall_score(y_test_r, y_pred_r)\\n",
    "        f1 = f1_score(y_test_r, y_pred_r)\\n",
    "        auc = roc_auc_score(y_test_r, model.predict_proba(X_test_scaled_r)[:, 1])\\n",
    "        results_clf_retard[name] = {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1-Score': f1, 'AUC': auc}\\n",
    "        \\n",
    "        print(f\\"Matrice de confusion :\\\\n{confusion_matrix(y_test_r, y_pred_r)}\\")\\n",
    "        \\n",
    "    # Afficher les résultats\\n",
    "    results_retard_df = pd.DataFrame(results_clf_retard).T\\n",
    "    print(\\"\\\\nTableau comparatif des performances pour la prédiction de RETARD :\\")\\n",
    "    display(results_retard_df.sort_values(by='AUC', ascending=False))\\n",
    "\\n",
    "    # Courbe ROC pour le meilleur modèle\\n",
    "    best_model_name = results_retard_df.sort_values(by='AUC', ascending=False).index[0]\\n",
    "    best_model = clf_models[best_model_name]\\n",
    "    y_pred_proba = best_model.predict_proba(X_test_scaled_r)[:,1]\\n",
    "    fpr, tpr, _ = roc_curve(y_test_r,  y_pred_proba)\\n",
    "    auc = roc_auc_score(y_test_r, y_pred_proba)\\n",
    "    plt.figure(figsize=(10,7))\\n",
    "    plt.plot(fpr,tpr,label=f'AUC = {auc:.2f}')\\n",
    "    plt.legend(loc=4)\\n",
    "    plt.title(f'Courbe ROC pour {best_model_name}')\\n",
    "    plt.show()\\n",
    "else:\\n",
    "    print(\\"Le DataFrame 'df' n'a pas été chargé.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Optimisation des Hyperparamètres avec GridSearchCV\\n",
    "\\n",
    "Pour améliorer encore les performances, nous pouvons optimiser les hyperparamètres du meilleur modèle. Nous allons le faire pour `RandomForestClassifier` sur la cible `RETARD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\\n",
    "    print(\\"Optimisation des hyperparamètres pour RandomForestClassifier (RETARD)...\\")\\n",
    "    \\n",
    "    # Définir la grille de paramètres\\n",
    "    param_grid = {\\n",
    "        'n_estimators': [100, 200],\\n",
    "        'max_depth': [10, 20],\\n",
    "        'min_samples_leaf': [1, 2]\\n",
    "    }\\n",
    "    \\n",
    "    # Instancier GridSearchCV\\n",
    "    # Note : Cette opération peut être très longue sur un grand jeu de données.\\n",
    "    grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42, n_jobs=-1), \\n",
    "                             param_grid=param_grid, \\n",
    "                             cv=3, \\n",
    "                             scoring='roc_auc', \\n",
    "                             verbose=2)\\n",
    "    \\n",
    "    # Exécuter la recherche sur un échantillon des données pour la démonstration\\n",
    "    grid_search.fit(X_train_resampled_r[:50000], y_train_resampled_r[:50000])\\n",
    "    \\n",
    "    print(f\\"\\\\nMeilleurs paramètres trouvés : {grid_search.best_params_}\\")\\n",
    "    print(f\\"Meilleur score AUC : {grid_search.best_score_:.4f}\\")\\n",
    "else:\\n",
    "    print(\\"Le DataFrame 'df' n'a pas été chargé.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Prédiction de la Résiliation (`RESILIE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\\n",
    "    # 1. Préparation des données pour la classification de 'RESILIE'\\n",
    "    print(\\"Préparation des données pour prédire RESILIE...\\")\\n",
    "    df_clf_resilie = df.copy()\\n",
    "    \\n",
    "    # Définir la cible et les features\\n",
    "    target_resilie = 'RESILIE'\\n",
    "    features_clf = [f for f in features if f not in ['RETARD', 'RESILIE']]\\n",
    "    \\n",
    "    X_resilie = df_clf_resilie[features_clf]\\n",
    "    y_resilie = df_clf_resilie[target_resilie]\\n",
    "\\n",
    "    # 2. Division des données\\n",
    "    X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_resilie, y_resilie, test_size=0.3, random_state=42, stratify=y_resilie)\\n",
    "\\n",
    "    # 3. Mise à l'échelle\\n",
    "    scaler_s = StandardScaler()\\n",
    "    X_train_scaled_s = scaler_s.fit_transform(X_train_s)\\n",
    "    X_test_scaled_s = scaler_s.transform(X_test_s)\\n",
    "\\n",
    "    # 4. Gestion du déséquilibre avec SMOTE\\n",
    "    print(\\"Application de SMOTE pour équilibrer les classes...\\")\\n",
    "    smote_s = SMOTE(random_state=42)\\n",
    "    X_train_resampled_s, y_train_resampled_s = smote_s.fit_resample(X_train_scaled_s, y_train_s)\\n",
    "\\n",
    "    # 5. Entraînement et évaluation des modèles\\n",
    "    results_clf_resilie = {}\\n",
    "    for name, model in clf_models.items(): # Réutiliser les mêmes modèles\\n",
    "        print(f\\"\\\\nEntraînement du modèle : {name}\\")\\n",
    "        model.fit(X_train_resampled_s, y_train_resampled_s)\\n",
    "        y_pred_s = model.predict(X_test_scaled_s)\\n",
    "        \\n",
    "        # Métriques\\n",
    "        acc = accuracy_score(y_test_s, y_pred_s)\\n",
        "        prec = precision_score(y_test_s, y_pred_s)\\n",
    "        rec = recall_score(y_test_s, y_pred_s)\\n",
    "        f1 = f1_score(y_test_s, y_pred_s)\\n",
    "        auc = roc_auc_score(y_test_s, model.predict_proba(X_test_scaled_s)[:, 1])\\n",
    "        results_clf_resilie[name] = {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1-Score': f1, 'AUC': auc}\\n",
    "        \\n",
    "        print(f\\"Matrice de confusion :\\\\n{confusion_matrix(y_test_s, y_pred_s)}\\")\\n",
    "        \\n",
    "    # Afficher les résultats\\n",
    "    results_resilie_df = pd.DataFrame(results_clf_resilie).T\\n",
    "    print(\\"\\\\nTableau comparatif des performances pour la prédiction de RESILIE :\\")\\n",
    "    display(results_resilie_df.sort_values(by='AUC', ascending=False))\\n",
    "\\n",
    "    # Courbe ROC pour le meilleur modèle\\n",
    "    best_model_name_s = results_resilie_df.sort_values(by='AUC', ascending=False).index[0]\\n",
    "    best_model_s = clf_models[best_model_name_s]\\n",
    "    y_pred_proba_s = best_model_s.predict_proba(X_test_scaled_s)[:,1]\\n",
    "    fpr_s, tpr_s, _ = roc_curve(y_test_s,  y_pred_proba_s)\\n",
    "    auc_s = roc_auc_score(y_test_s, y_pred_proba_s)\\n",
    "    plt.figure(figsize=(10,7))\\n",
    "    plt.plot(fpr_s,tpr_s,label=f'AUC = {auc_s:.2f}')\\n",
    "    plt.legend(loc=4)\\n",
    "    plt.title(f'Courbe ROC pour {best_model_name_s}')\\n",
    "    plt.show()\\n",
    "else:\\n",
    "    print(\\"Le DataFrame 'df' n'a pas été chargé.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering pour la Segmentation des Clients\\n",
    "\\n",
    "Nous allons maintenant utiliser l'algorithme K-Means pour segmenter les clients en groupes homogènes en fonction de leur comportement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\\n",
    "    # 1. Sélection des variables pour le clustering\\n",
    "    print(\\"Préparation des données pour le clustering...\\")\\n",
    "    cluster_features = ['CUBCONS', 'MONT-TTC', 'DELAI_REGL', 'RETARD']\\n",
    "    df_cluster = df[cluster_features].copy()\\n",
    "    \\n",
    "    # 2. Mise à l'échelle des données\\n",
    "    scaler_cluster = StandardScaler()\\n",
    "    df_cluster_scaled = scaler_cluster.fit_transform(df_cluster)\\n",
    "    \\n",
    "    # 3. Détermination du nombre optimal de clusters (Méthode du coude)\\n",
    "    print(\\"Détermination du nombre optimal de clusters avec la méthode du coude...\\")\\n",
    "    inertia = []\\n",
    "    k_range = range(2, 11)\\n",
    "    for k in k_range:\\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\\n",
    "        kmeans.fit(df_cluster_scaled)\\n",
    "        inertia.append(kmeans.inertia_)\\n",
    "    \\n",
    "    # Visualisation de la méthode du coude\\n",
    "    plt.figure(figsize=(10, 6))\\n",
    "    plt.plot(k_range, inertia, marker='o', linestyle='--')\\n",
    "    plt.xlabel('Nombre de clusters (k)')\\n",
    "    plt.ylabel('Inertie')\\n",
    "    plt.title('Méthode du coude pour déterminer k')\\n",
    "    plt.xticks(k_range)\\n",
    "    plt.grid(True)\\n",
    "    plt.show()\\n",
    "    \\n",
    "    # 4. Application de K-Means avec le k optimal (ex: k=4)\\n",
    "    optimal_k = 4 # À ajuster en fonction du graphique ci-dessus\\n",
    "    print(f\\"\\\\nApplication de K-Means avec k={optimal_k} clusters...\\")\\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\\n",
    "    df['Cluster'] = kmeans.fit_predict(df_cluster_scaled)\\n",
    "    \\n",
    "    # 5. Analyse des segments\\n",
    "    print(\\"\\\\nAnalyse des caractéristiques des segments de clients :\\")\\n",
    "    cluster_profile = df.groupby('Cluster')[cluster_features].mean()\\n",
    "    display(cluster_profile)\\n",
    "    \\n",
    "    # Visualisation des clusters\\n",
    "    print(\\"\\\\nVisualisation des clusters :\\")\\n",
    "    fig = px.scatter(df.sample(50000), x='MONT-TTC', y='CUBCONS', color='Cluster',\\n",
    "                     title='Segmentation des clients par K-Means',\\n",
    "                     labels={'MONT-TTC': 'Montant Total TTC', 'CUBCONS': 'Consommation (m³)'},\\n",
    "                     hover_data=['DELAI_REGL', 'RETARD'])\\n",
    "    fig.show()\\n",
    "else:\\n",
    "    print(\\"Le DataFrame 'df' n'a pas été chargé.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sélection et Interprétation des Variables\\n",
    "\\n",
    "Comprendre quelles variables influencent le plus nos prédictions est crucial pour formuler des recommandations pertinentes. Nous utiliserons les résultats de nos modèles pour identifier ces variables clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clf_models' in locals() and 'models' in locals():\\n",
    "    # 1. Importance des variables avec Random Forest (pour la classification)\\n",
    "    print(\\"Importance des variables pour la prédiction de 'RETARD' (Random Forest) :\\")\\n",
    "    rf_retard = clf_models['Random Forest'] # Récupérer le modèle entraîné\\n",
    "    feature_importances_retard = pd.DataFrame(rf_retard.feature_importances_, index=features_clf, columns=['Importance']).sort_values('Importance', ascending=False)\\n",
    "    \\n",
    "    plt.figure(figsize=(12, 8))\\n",
    "    sns.barplot(x=feature_importances_retard.Importance, y=feature_importances_retard.index)\\n",
    "    plt.title('Importance des variables pour la prédiction de RETARD')\\n",
    "    plt.show()\\n",
    "\\n",
    "    # 2. Sélection de variables avec les coefficients de Lasso (pour la régression)\\n",
    "    print(\\"\\\\nImportance des variables pour la prédiction de 'MONT-FDE' (Lasso) :\\")\\n",
    "    lasso_model = models['Lasso']\\n",
    "    lasso_coeffs = pd.DataFrame(lasso_model.coef_, index=features, columns=['Coefficient']).sort_values('Coefficient', ascending=False)\\n",
    "    \\n",
    "    # Filtrer les coefficients non nuls\\n",
    "    lasso_coeffs_filtered = lasso_coeffs[lasso_coeffs['Coefficient'] != 0]\\n",
    "\\n",
    "    plt.figure(figsize=(12, 8))\\n",
    "    sns.barplot(x=lasso_coeffs_filtered.Coefficient, y=lasso_coeffs_filtered.index)\\n",
    "    plt.title('Coefficients du modèle Lasso pour MONT-FDE (variables non nulles)')\\n",
    "    plt.show()\\n",
    "else:\\n",
    "    print(\\"Les modèles n'ont pas encore été entraînés. Veuillez exécuter les étapes 4 et 5.\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Synthèse, Conclusion et Recommandations\\n",
    "\\n",
    "Cette section finale consolide les résultats de notre analyse pour répondre aux questions métier initiales et proposer des recommandations opérationnelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Résumé des Résultats\\n",
    "\\n",
    "1.  **Analyse Exploratoire** : Nous avons observé une tendance [stable/à la hausse/à la baisse] du `MONT-FDE` global. L'analyse a révélé que la majorité des clients sont de type `PRIVE` et que certaines régions (`DR`) contribuent plus significativement aux revenus. Les retards de paiement et les résiliations sont des phénomènes [rares/fréquents], indiquant un potentiel déséquilibre des classes pour la modélisation.\\n",
    "\\n",
    "2.  **Modélisation de Régression (`MONT-FDE`)** : Le modèle [Nom du meilleur modèle, ex: Ridge] a montré la meilleure performance avec un R² de [valeur]. Les variables les plus importantes pour prédire le `MONT-FDE` étaient [ex: `CUBCONS`, `MONT-TTC`], ce qui confirme que la consommation et le montant total de la facture sont des prédicteurs logiques.\\n",
    "\\n",
    "3.  **Modélisation de Classification (`RETARD` et `RESILIE`)** : Le modèle [Nom du meilleur modèle, ex: XGBoost] a obtenu les meilleures performances pour prédire à la fois le retard de paiement et la résiliation, avec des scores AUC de [valeur] et [valeur] respectivement. Les facteurs clés influençant le retard de paiement semblent être [ex: `DELAI_REGL`, `MONT-TTC`], tandis que la résiliation est plus liée à [ex: `Ancienneté`, `DIAM`].\\n",
    "\\n",
    "4.  **Segmentation des Clients** : Nous avons identifié [nombre] segments de clients distincts. Par exemple :\\n",
    "    *   **Cluster 0 - Gros Consommateurs Ponctuels** : Clients avec une consommation et des factures élevées, mais qui paient généralement à temps.\\n",
    "    *   **Cluster 1 - Petits Consommateurs à Risque** : Faible consommation, mais tendance aux retards de paiement.\\n",
    "    *   *(Adaptez les descriptions en fonction des résultats réels du clustering)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Réponses aux Questions Métier\\n",
    "\\n",
    "**1. Comment évolue le FDE depuis 2014 ?**\\n",
    "   - L'analyse temporelle a montré que [décrire la tendance observée dans le graphique de l'EDA]. Bien que les volumes d'eau distribuée aient augmenté, le revenu FDE n'a pas suivi la même progression, suggérant des problèmes de facturation ou de recouvrement.\\n",
    "\\n",
    "**2. Quels sont les contributeurs du FDE ?**\\n",
    "   - Les clients de type `PRIVE` représentent la majorité des revenus FDE. Au niveau géographique, les régions [lister les DR les plus importantes] sont les plus grands contributeurs.\\n",
    "\\n",
    "**3. Les habitudes des clients ont-elles changé ?**\\n",
    "   - Oui, nos modèles prédictifs et notre segmentation suggèrent que certains groupes de clients sont devenus plus enclins au retard de paiement. Le segment [ex: Cluster 1] a été identifié comme un groupe à risque. Les facteurs comme [citer une variable importante] sont de bons indicateurs de ce changement de comportement.\\n",
    "\\n",
    "**4. Quels facteurs expliquent la baisse des ressources FDE ?**\\n",
    "   - La baisse n'est pas uniquement due à une diminution de la consommation. Les principaux facteurs semblent être une combinaison de :\\n",
    "     - **Retards de paiement croissants** : Comme le montre notre modèle de classification, un nombre significatif de factures sont payées en retard, ce qui impacte la trésorerie.\\n",
    "     - **Inefficacités dans la facturation ou le recouvrement** : La décorrélation entre l'augmentation des volumes distribués et la stagnation des revenus FDE pointe vers ce problème.\\n",
    "     - **Caractéristiques spécifiques des clients** : Des variables comme le délai de règlement (`DELAI_REGL`) et le montant total de la facture (`MONT-TTC`) sont fortement prédictives du non-paiement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Recommandations Opérationnelles\\n",
    "\\n",
    "Sur la base de cette analyse, nous proposons les actions suivantes :\\n",
    "\\n",
    "1.  **Mettre en place des stratégies de recouvrement ciblées** :\\n",
    "    *   **Action** : Utiliser le modèle de prédiction de `RETARD` pour identifier en amont les clients susceptibles de ne pas payer à temps. Contacter proactivement les clients du **Segment à Risque** (ex: Cluster 1) avec des rappels de paiement personnalisés (SMS, email).\\n",
    "    *   **Justification** : Une action préventive est plus efficace et moins coûteuse qu'un recouvrement tardif.\\n",
    "\\n",
    "2.  **Optimiser la facturation pour les gros consommateurs** :\\n",
    "    *   **Action** : Analyser en profondeur les clients du segment **\\"Gros Consommateurs\\"** (ex: Cluster 0). Assurer que leur facturation est précise et qu'il n'y a pas de pertes techniques (fuites, compteurs défectueux) ou commerciales (fraude).\\n",
    "    *   **Justification** : Une petite amélioration sur ce segment à fort volume peut avoir un impact significatif sur les revenus FDE.\\n",
    "\\n",
    "3.  **Lancer des campagnes de fidélisation pour prévenir la résiliation** :\\n",
    "    *   **Action** : Utiliser le modèle de prédiction de `RESILIE` pour identifier les clients présentant un risque de départ. Les variables comme [citer une variable importante, ex: ancienneté] peuvent servir de déclencheurs pour contacter ces clients et leur proposer des offres ou des services améliorés.\\n",
    "    *   **Justification** : Conserver un client existant est souvent plus rentable que d'en acquérir un nouveau.\\n",
    "\\n",
    "4.  **Approfondir l'analyse par zone géographique** :\\n",
    "    *   **Action** : Mener des investigations spécifiques dans les régions (`DR`) où les taux de retard de paiement sont les plus élevés afin de comprendre les causes locales (problèmes économiques, qualité de service, etc.).\\n",
    "    *   **Justification** : Les problèmes peuvent être localisés et nécessiter des solutions adaptées à chaque contexte régional."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
